# Overview
In this repository, we introduce QuanTemp++: a dataset consisting of natural numerical claims, an open domain corpus, and the corresponding evidence relevance and veracity labels. Given this dataset, we also aim to characterize the retrieval performance of key query planning paradigms, especially those of decomposition as they have shown promising results in other tasks. Finally, we observe their effect on the outcome of the verification pipeline and draw insights.
# Dataset Statistics

| Split       | TRUE | FALSE | CONFLICTING | Total |
|--------------|------|--------|--------------|--------|
| Train        | 1824 | 5770   | 2341         | 9935   |
| Validation   | 617  | 1759   | 672          | 3084   |
| Test         | 474  | 1423   | 598          | 2495   |
| **Total**    |      |        |              | **15514** |

**Table 1:** Distribution of claims by veracity label


| Split       | Temporal | Interval | Statistical | Comparison |
|--------------|-----------|-----------|--------------|-------------|
| Train        | 2672      | 1541      | 4660         | 1051        |
| Validation   | 840       | 469       | 1432         | 339         |
| Test         | 681       | 347       | 1210         | 255         |
| **Total**    | **4193**  | **2357**  | **7302**     | **1645**    |
| **%**        | 27.06     | 15.21     | 47.12        | 10.61       |

**Table 2:** Distribution of claims by numerical abilities required


# Data Creation Pipeline
![data_creation_pipe](https://github.com/user-attachments/assets/f126c339-821e-4c97-8bbb-87c790675959)

# Dataset

The dataset artifacts can be found in the below link:
[Dataset files](https://drive.google.com/file/d/1WUS9Zt8gJDAs2igHjh2mVRj92acK_9P4/view?usp=sharing)

- The claims, their veracity labels and metadata are available in the files {train/test/val}.json. This file follows the format of QuanTemp and has the following extended fields:
  - decomp_questions: queries generated by ClaimDecomp for the claim
  - pgmfc_questions: queries generated by PgmFC for the claim
  - in_file: oracle queries generated by the data pipeline for the claim
  - google_search: google search results per query for the claims
  - filtered_search_results: search results after filtering for leakage.
- The queries generated for the test claims using our trained FlanT5 model are present at qgen_out.csv
- The open domain corpus can be found in corpus.json. The pre-indexed corpus for the contriever retriever can be found in contiever_corpus.index
- The mapping of relevant corpus items to the claims of the corresponding split can be found in {train/test/val}_qrels.json


# Project structure

* src
  * data_pipe : scripts used to create the dataset
      * agq : scripts used to generate queries for the claim
      * serp: scripts used to fetch google search results for each query of the claim
      * crawl_links: scripts used to crawl the web for the full web page content of the results
      * relevance: filters used by different components to remove temporal and gold leakage.
  * evaluation/retrieval : files used to retrieve relevant items for the claim by various retrievers and evaluate their performance.
      * Retriever.py : The Contriever, BiEncoder and its various versions with temporal filter are available in this class.
      * files prefixed with bm25 evaluate retrieval using bm25 and the remaining are tuned to use the respective retriever class.
  * nli : infers the veracity label given the claim and the retrieved evidence and evaluates the performance
      * baseline : gpt prompting files that generated the veracity label
      * train: files to fine tune the roberta-large-mnli model
      * infer: files to infer from the fine tuned roberta-large-mnli model 
      * evaluate: takes the nli results and evaluates the performance
  * qgen : files to train and infer from the FlanT5 model for automatic query generations.

 # Setup
 Pull the repository then:
 ``` conda create --name <env> --file requirements.txt ```

  
